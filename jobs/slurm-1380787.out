cpu-bind=MASK - bp1-gpu026, task 4294967295 4294967295 [0]: mask 0x101 set
==>> input params: ['gpus=0,1,2,3', 'backbone=Swin-m', 'num_epochs=100', 'exp_id=Swin_t_pretrained_yolo_m', 'use_amp=True', 'val_intervals=2', 'data_num_workers=14', 'batch_size=64', 'random_size=[14,26]', 'input_size=[640,640]', 'test_size=[640,640]', 'swin_pretrained=True', 'swin_weights_path=weights/swin_t.pth']
[INFO] change param: gpus 0 -> (0, 1, 2, 3) ('tuple')
[INFO] change param: backbone CSPDarknet-s -> Swin-m ('str')
[INFO] change param: num_epochs 300 -> 100 ('int')
[INFO] change param: exp_id gputest1 -> Swin_t_pretrained_yolo_m ('str')
[INFO] change param: use_amp False -> True ('bool')
[INFO] same param: val_intervals=2 ('int')
[INFO] change param: data_num_workers 4 -> 14 ('int')
[INFO] change param: batch_size 24 -> 64 ('int')
[INFO] change param: random_size None -> [14, 26] ('list')
[INFO] same param: input_size=[640, 640] ('list')
[INFO] same param: test_size=[640, 640] ('list')
[INFO] change param: swin_pretrained False -> True ('bool')
[INFO] change param: swin_weights_path None -> weights/swin_t.pth ('str')
[INFO] re-change param: gpus [0, 1, 2, 3] to 0,1,2,3 'str' 

-------------------- final config: --------------------
{'exp_id': 'Swin_t_pretrained_yolo_m', 'dataset_path': '/user/home/bq18557/scratch/COCO', 'backbone': 'Swin-m', 'input_size': [640, 640], 'random_size': [14, 26], 'test_size': [640, 640], 'gpus': [0, 1, 2, 3], 'batch_size': 64, 'master_batch_size': 16, 'num_epochs': 100, 'swin_pretrained': True, 'swin_weights_path': 'weights/swin_t.pth', 'label_name': ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'], 'reid_dim': 0, 'tracking_id_nums': None, 'warmup_lr': 0, 'basic_lr_per_img': 0.00015625, 'scheduler': 'yoloxwarmcos', 'no_aug_epochs': 15, 'min_lr_ratio': 0.05, 'weight_decay': 0.0005, 'warmup_epochs': 5, 'depth_wise': False, 'stride': [8, 16, 32], 'degrees': 10.0, 'translate': 0.1, 'scale': [0.1, 2], 'shear': 2.0, 'perspective': 0.0, 'enable_mixup': True, 'seed': None, 'mosaic_prob': 1.0, 'mixup_prob': 1.0, 'data_num_workers': 14, 'momentum': 0.9, 'vis_thresh': 0.3, 'load_model': '', 'ema': True, 'grad_clip': {'max_norm': 35, 'norm_type': 2}, 'print_iter': 10, 'val_intervals': 2, 'save_epoch': 5, 'resume': False, 'use_amp': True, 'cuda_benchmark': False, 'nms_thresh': 0.65, 'occupy_mem': False, 'cache': False, 'rgb_means': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225], 'train_ann': '/user/home/bq18557/scratch/COCO/annotations/instances_train2017.json', 'val_ann': '/user/home/bq18557/scratch/COCO/annotations/instances_val2017.json', 'data_dir': '/user/home/bq18557/scratch/COCO', 'num_classes': 80, 'gpus_str': '0,1,2,3', 'chunk_sizes': [16, 16, 16, 16], 'root_dir': '/user/home/bq18557/Transformer-YOLOX', 'save_dir': '/user/home/bq18557/Transformer-YOLOX/exp/Swin_t_pretrained_yolo_m'}
log file will be saved to /user/home/bq18557/Transformer-YOLOX/exp/Swin_t_pretrained_yolo_m/logs_2022-04-05-17-25/log.txt
Used pretrained model parameters:dict_keys(['patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed.norm.weight', 'patch_embed.norm.bias', 'layers.0.blocks.0.norm1.weight', 'layers.0.blocks.0.norm1.bias', 'layers.0.blocks.0.attn.relative_position_bias_table', 'layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.attn.qkv.weight', 'layers.0.blocks.0.attn.qkv.bias', 'layers.0.blocks.0.attn.proj.weight', 'layers.0.blocks.0.attn.proj.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.0.mlp.fc1.weight', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.weight', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.1.norm1.weight', 'layers.0.blocks.1.norm1.bias', 'layers.0.blocks.1.attn.relative_position_bias_table', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.attn.qkv.weight', 'layers.0.blocks.1.attn.qkv.bias', 'layers.0.blocks.1.attn.proj.weight', 'layers.0.blocks.1.attn.proj.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.0.blocks.1.mlp.fc1.weight', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.weight', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.downsample.reduction.weight', 'layers.0.downsample.norm.weight', 'layers.0.downsample.norm.bias', 'layers.1.blocks.0.norm1.weight', 'layers.1.blocks.0.norm1.bias', 'layers.1.blocks.0.attn.relative_position_bias_table', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.qkv.weight', 'layers.1.blocks.0.attn.qkv.bias', 'layers.1.blocks.0.attn.proj.weight', 'layers.1.blocks.0.attn.proj.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.0.mlp.fc1.weight', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.weight', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.1.norm1.weight', 'layers.1.blocks.1.norm1.bias', 'layers.1.blocks.1.attn.relative_position_bias_table', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.attn.qkv.weight', 'layers.1.blocks.1.attn.qkv.bias', 'layers.1.blocks.1.attn.proj.weight', 'layers.1.blocks.1.attn.proj.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.1.blocks.1.mlp.fc1.weight', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.weight', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.downsample.reduction.weight', 'layers.1.downsample.norm.weight', 'layers.1.downsample.norm.bias', 'layers.2.blocks.0.norm1.weight', 'layers.2.blocks.0.norm1.bias', 'layers.2.blocks.0.attn.relative_position_bias_table', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.qkv.weight', 'layers.2.blocks.0.attn.qkv.bias', 'layers.2.blocks.0.attn.proj.weight', 'layers.2.blocks.0.attn.proj.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.0.mlp.fc1.weight', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.weight', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.1.norm1.weight', 'layers.2.blocks.1.norm1.bias', 'layers.2.blocks.1.attn.relative_position_bias_table', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.attn.qkv.weight', 'layers.2.blocks.1.attn.qkv.bias', 'layers.2.blocks.1.attn.proj.weight', 'layers.2.blocks.1.attn.proj.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.1.mlp.fc1.weight', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.weight', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.2.norm1.weight', 'layers.2.blocks.2.norm1.bias', 'layers.2.blocks.2.attn.relative_position_bias_table', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.attn.qkv.weight', 'layers.2.blocks.2.attn.qkv.bias', 'layers.2.blocks.2.attn.proj.weight', 'layers.2.blocks.2.attn.proj.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.weight', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.weight', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.norm1.weight', 'layers.2.blocks.3.norm1.bias', 'layers.2.blocks.3.attn.relative_position_bias_table', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.attn.qkv.weight', 'layers.2.blocks.3.attn.qkv.bias', 'layers.2.blocks.3.attn.proj.weight', 'layers.2.blocks.3.attn.proj.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.weight', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.weight', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm1.weight', 'layers.2.blocks.4.norm1.bias', 'layers.2.blocks.4.attn.relative_position_bias_table', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.attn.qkv.weight', 'layers.2.blocks.4.attn.qkv.bias', 'layers.2.blocks.4.attn.proj.weight', 'layers.2.blocks.4.attn.proj.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.weight', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.weight', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.blocks.5.norm1.weight', 'layers.2.blocks.5.norm1.bias', 'layers.2.blocks.5.attn.relative_position_bias_table', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.attn.qkv.weight', 'layers.2.blocks.5.attn.qkv.bias', 'layers.2.blocks.5.attn.proj.weight', 'layers.2.blocks.5.attn.proj.bias', 'layers.2.blocks.5.norm2.weight', 'layers.2.blocks.5.norm2.bias', 'layers.2.blocks.5.mlp.fc1.weight', 'layers.2.blocks.5.mlp.fc1.bias', 'layers.2.blocks.5.mlp.fc2.weight', 'layers.2.blocks.5.mlp.fc2.bias', 'layers.2.downsample.reduction.weight', 'layers.2.downsample.norm.weight', 'layers.2.downsample.norm.bias', 'layers.3.blocks.0.norm1.weight', 'layers.3.blocks.0.norm1.bias', 'layers.3.blocks.0.attn.relative_position_bias_table', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.attn.qkv.weight', 'layers.3.blocks.0.attn.qkv.bias', 'layers.3.blocks.0.attn.proj.weight', 'layers.3.blocks.0.attn.proj.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.0.mlp.fc1.weight', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.weight', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.1.norm1.weight', 'layers.3.blocks.1.norm1.bias', 'layers.3.blocks.1.attn.relative_position_bias_table', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.attn.qkv.weight', 'layers.3.blocks.1.attn.qkv.bias', 'layers.3.blocks.1.attn.proj.weight', 'layers.3.blocks.1.attn.proj.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'layers.3.blocks.1.mlp.fc1.weight', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.weight', 'layers.3.blocks.1.mlp.fc2.bias', 'norm0.weight', 'norm0.bias', 'norm1.weight', 'norm1.bias', 'norm2.weight', 'norm2.bias', 'norm3.weight', 'norm3.bias'])
Swin-T weights loaded and frozen
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
YOLOX                                                   --
├─SwinTransformer: 1-1                                  --
│    └─PatchEmbed: 2-1                                  --
│    │    └─Conv2d: 3-1                                 (4,704)
│    │    └─LayerNorm: 3-2                              (192)
│    └─Dropout: 2-2                                     --
│    └─ModuleList: 2-3                                  --
│    │    └─BasicLayer: 3-3                             (299,190)
│    │    └─BasicLayer: 3-4                             (1,188,204)
│    │    └─BasicLayer: 3-5                             (11,841,672)
│    │    └─BasicLayer: 3-6                             (14,183,856)
│    └─LayerNorm: 2-4                                   (192)
│    └─LayerNorm: 2-5                                   (384)
│    └─LayerNorm: 2-6                                   (768)
│    └─LayerNorm: 2-7                                   (1,536)
├─YOLOXPAFPN: 1-2                                       --
│    └─Upsample: 2-8                                    --
│    └─BaseConv: 2-9                                    --
│    │    └─Conv2d: 3-7                                 165,888
│    │    └─BatchNorm2d: 3-8                            576
│    │    └─SiLU: 3-9                                   --
│    └─CSPLayer: 2-10                                   --
│    │    └─BaseConv: 3-10                              83,232
│    │    └─BaseConv: 3-11                              83,232
│    │    └─BaseConv: 3-12                              83,520
│    │    └─Sequential: 3-13                            415,872
│    └─BaseConv: 2-11                                   --
│    │    └─Conv2d: 3-14                                41,472
│    │    └─BatchNorm2d: 3-15                           288
│    │    └─SiLU: 3-16                                  --
│    └─CSPLayer: 2-12                                   --
│    │    └─BaseConv: 3-17                              20,880
│    │    └─BaseConv: 3-18                              20,880
│    │    └─BaseConv: 3-19                              21,024
│    │    └─Sequential: 3-20                            104,256
│    └─BaseConv: 2-13                                   --
│    │    └─Conv2d: 3-21                                186,624
│    │    └─BatchNorm2d: 3-22                           288
│    │    └─SiLU: 3-23                                  --
│    └─CSPLayer: 2-14                                   --
│    │    └─BaseConv: 3-24                              41,760
│    │    └─BaseConv: 3-25                              41,760
│    │    └─BaseConv: 3-26                              83,520
│    │    └─Sequential: 3-27                            415,872
│    └─BaseConv: 2-15                                   --
│    │    └─Conv2d: 3-28                                746,496
│    │    └─BatchNorm2d: 3-29                           576
│    │    └─SiLU: 3-30                                  --
│    └─CSPLayer: 2-16                                   --
│    │    └─BaseConv: 3-31                              166,464
│    │    └─BaseConv: 3-32                              166,464
│    │    └─BaseConv: 3-33                              332,928
│    │    └─Sequential: 3-34                            1,661,184
├─YOLOXHead: 1-3                                        --
│    └─ModuleList: 2-17                                 --
│    │    └─BaseConv: 3-35                              28,032
│    │    └─BaseConv: 3-36                              55,680
│    │    └─BaseConv: 3-37                              110,976
│    └─ModuleList: 2-18                                 --
│    │    └─Sequential: 3-38                            664,320
│    │    └─Sequential: 3-39                            664,320
│    │    └─Sequential: 3-40                            664,320
│    └─ModuleList: 2-19                                 --
│    │    └─Sequential: 3-41                            664,320
│    │    └─Sequential: 3-42                            664,320
│    │    └─Sequential: 3-43                            664,320
│    └─ModuleList: 2-20                                 --
│    │    └─Conv2d: 3-44                                15,440
│    │    └─Conv2d: 3-45                                15,440
│    │    └─Conv2d: 3-46                                15,440
│    └─ModuleList: 2-21                                 --
│    │    └─Conv2d: 3-47                                772
│    │    └─Conv2d: 3-48                                772
│    │    └─Conv2d: 3-49                                772
│    └─ModuleList: 2-22                                 --
│    │    └─Conv2d: 3-50                                193
│    │    └─Conv2d: 3-51                                193
│    │    └─Conv2d: 3-52                                193
├─YOLOXLoss: 1-4                                        --
│    └─L1Loss: 2-23                                     --
│    └─BCEWithLogitsLoss: 2-24                          --
│    └─IOUloss: 2-25                                    --
================================================================================
Total params: 36,612,255
Trainable params: 9,114,879
Non-trainable params: 27,497,376
================================================================================
creating data loaders, start time: 1649175918.4558508
==> Loading train2017 annotation /user/home/bq18557/scratch/COCO/annotations/instances_train2017.json
loading annotations into memory...
Done (t=15.70s)
creating index...
index created!
images number 118287
==> Loading val2017 annotation /user/home/bq18557/scratch/COCO/annotations/instances_val2017.json
loading annotations into memory...
Done (t=1.59s)
creating index...
index created!
images number 5000
classes index: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]
class names in dataset: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']
data loaders created in: 50.03419303894043
shuffle images list in /user/home/bq18557/scratch/COCO/annotations/instances_train2017.json
multi size training: [[448, 448], [480, 480], [512, 512], [544, 544], [576, 576], [608, 608], [640, 640], [672, 672], [704, 704], [736, 736], [768, 768], [800, 800]]
/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Traceback (most recent call last):
  File "train.py", line 213, in <module>
    main()
  File "train.py", line 200, in main
    train(model, scaler, train_loader, val_loader, optimizer, lr_scheduler, start_epoch, no_aug)
  File "train.py", line 123, in train
    loss_dict_train, _ = run_epoch(model, optimizer, scaler, ema, "train", epoch, train_loader,
  File "train.py", line 50, in run_epoch
    _, loss_stats = model_with_loss(inps, targets=targets)
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/bq18557/Transformer-YOLOX/models/yolox.py", line 109, in forward
    neck_feats = self.neck(body_feats)
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/bq18557/Transformer-YOLOX/models/neck/yolo_fpn.py", line 891, in forward
    fpn_out0 = self.lateral_conv0(x0)  # 1024->512/32
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/bq18557/Transformer-YOLOX/models/backbone/csp_darknet.py", line 169, in forward
    return self.act(self.bn(self.conv(x)))
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/user/home/bq18557/work-env/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 443, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [288, 576, 1, 1], expected input[16, 768, 25, 25] to have 576 channels, but got 768 channels instead

